{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95919a3-3026-4de0-afb3-4b5eb99b81a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<start> و از روی گزارشاتی که برای رؤسای من در قاهره ارسال گردیده بودند نوشته شد. <end>',\n",
       " '<start> from notes jotted daily on the march, strengthened by some reports sent to my chiefs in Cairo. <end>')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from TextProcessing import Encode, Decode, NaturalLanguageProcessing, Attention\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "step_per_epoch = len(x_train) // BATCH_SIZE\n",
    "HM_UNITS_ENCODE = 64\n",
    "HM_UNITS_DECODE = 64\n",
    "embending_dim = 256\n",
    "vocab_inp_size = len(input_key.word_index) + 1\n",
    "vocab_out_size = len(output_key.word_index) + 1\n",
    "\n",
    "persian_path = 'mizan/mizan_fa.txt'\n",
    "english_path = 'mizan/mizan_en.txt'\n",
    "\n",
    "def prepare_data(sentense):\n",
    "    sentense = re.sub(r'[\\n]', r'', sentense)\n",
    "    sentense = re.sub(r'[?$*@#]', r'', sentense)\n",
    "    sentense = '<start> ' + sentense + ' <end>'\n",
    "    return sentense\n",
    "\n",
    "def ReadText(path, maxlen=1000):\n",
    "    lst = []\n",
    "    with open(path, 'r', encoding='UTF-8') as file:\n",
    "        lines = file.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            line = prepare_data(line)\n",
    "            lst.append(line)\n",
    "            if i==maxlen:\n",
    "                break\n",
    "    \n",
    "    return lst\n",
    "\n",
    "persian = ReadText(persian_path)\n",
    "english = ReadText(english_path)    \n",
    "persian[1], english[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f8b47c-ad37-4a96-bb49-9d8e141ac86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train (880, 88)\n",
      "Shape of the Y_train (880, 64)\n",
      "Shape of the X_valid (121, 88)\n",
      "Shape of the Y_valid (121, 64)\n"
     ]
    }
   ],
   "source": [
    "nlp = NaturalLanguageProcessing(persian, english)\n",
    "input_value, output_value, input_key, output_key = nlp.TokenizerText()\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_value, output_value, test_size=0.12)\n",
    "print(f\"Shape of X_train {x_train.shape}\")\n",
    "print(f\"Shape of the Y_train {y_train.shape}\")\n",
    "print(f\"Shape of the X_valid {x_valid.shape}\")\n",
    "print(f\"Shape of the Y_valid {y_valid.shape}\")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train))\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "348f4577-baaf-4cb6-a938-caa988311bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  115 2652   17    7 2653    6 2654    4  611    1 1379    5   95\n",
      " 2655  292 2656 1213    3    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0] \n",
      "\n",
      "[   2   23   41 1948  965   53    1  585 1949    4  307    5 1950    3\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0], '\\n') #this is persian data\n",
    "print(y_train[0]) #this is english data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5caa2b28-a84e-4826-b4fe-8873969d6194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Encode at 0x13d88de72b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encode(vocab_inp_size, embending_dim, HM_UNITS_ENCODE, BATCH_SIZE)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22060974-fc7b-40d8-a12b-02271f889d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Decode at 0x13d88de66b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode = Decode(vocab_out_size, embending_dim, HM_UNITS_DECODE, BATCH_SIZE)\n",
    "decode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8759e56f-90c1-4d3d-94a8-c3ab1a44bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0 Total Loss is 114.2126235961914\n",
      "Epochs 1 Total Loss is 97.24425506591797\n",
      "Epochs 2 Total Loss is 94.83502197265625\n",
      "Epochs 3 Total Loss is 93.40696716308594\n",
      "Epochs 4 Total Loss is 92.34122467041016\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=opt, encoder=encoder, decoder=decode)\n",
    "\n",
    "def LossFunc(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def train_step(inp, out, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as grad:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([output_key.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        for t in range(1, out.shape[1]):\n",
    "            predictions, dec_hidden, _ = decode(dec_input, dec_hidden, enc_output)\n",
    "            loss += LossFunc(out[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(out[:, t], 1)\n",
    "        batch_loss = (loss / int(out.shape[1]))\n",
    "        variables = encoder.trainable_variables + decode.trainable_variables\n",
    "        gradients = grad.gradient(loss, variables)\n",
    "        opt.apply_gradients(zip(gradients, variables))\n",
    "        return batch_loss\n",
    "    \n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.InitialHiddenState()\n",
    "    total_loss = 0\n",
    "    for inp, out in dataset.take(step_per_epoch):\n",
    "        batch_loss = train_step(inp, out, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    print(f'Epochs {epoch} Total Loss is {total_loss.numpy()}')\n",
    "    checkpoint.save(file_prefix='test_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9daff25-e7b9-4193-bf49-6b23f8825e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sentense):\n",
    "    sentense = re.sub(r'[\\n]', r'', sentense)\n",
    "    sentense = re.sub(r'[?$*@#]', r'', sentense)\n",
    "    sentense = '<start> ' + sentense + ' <end>'\n",
    "    return sentense\n",
    "\n",
    "def evaluate(sentence):\n",
    "    def MaxLength(tensore):\n",
    "        return max(len(t) for t in tensore)\n",
    "    \n",
    "    max_len_inp = MaxLength(input_value)\n",
    "    max_len_out = MaxLength(output_value)\n",
    "    prepare_data(sentence)\n",
    "    inputs = [input_key.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_len_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    results = ''\n",
    "    hidden = [tf.zeros((1, HM_UNITS_ENCODE))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([output_key.word_index['<start>']], 0)\n",
    "    for t in range(max_len_out):\n",
    "        predictions, dec_hidden, waigth = decode(dec_input, dec_hidden, enc_out)\n",
    "        waigth = tf.reshape(waigth, (-1, ))\n",
    "        prediced_id = tf.argmax(predictions[0]).numpy()\n",
    "        results += output_key.index_word[prediced_id] + ' '\n",
    "        if output_key.index_word[prediced_id]=='<end>':\n",
    "            return results, sentence\n",
    "        dec_input = tf.expand_dims([prediced_id], 0)\n",
    "\n",
    "        return results, sentence\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc7c4c2b-ec5e-4404-9000-f82796f0d809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x13d977e1270>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0651263f-f1a5-4455-805e-2a80eeafb5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Woman of freedom life', 'زن زندگی آزادی .')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = \"زن زندگی آزادی .\"\n",
    "evaluate(text_test) # translate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a483c-8f59-4969-9d9d-f94f4082e70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cd427-aae4-4ced-b4e9-ee707148c7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9660a85-b0aa-442b-b32d-e6d9b002164d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b9ebd-3556-46b6-a6ce-217edf994162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
